{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_variant = 'bert-base-chinese'\n",
    "bert_variant = 'hfl/chinese-roberta-wwm-ext'\n",
    "freeze_bert = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class    fold0    fold1    fold2    fold3    fold4    test\n",
      "-------  -------  -------  -------  -------  -------  ------\n",
      "      0      160      160      160      160      159     199\n",
      "      1      160      160      160      160      160     202\n",
      "['高考理科录取人数最多20个主流大众专业解析' '中央民族大学2009年普通本科招生章程' '研究生考试迟到15分钟不得入场'\n",
      " '广州中考体育12日报名考生签名确认成绩' '09成考时间17日至18日 11月上旬划定分数线']\n",
      "['教育部就中小学教师队伍补充等有关工作答问' '安理会半数成员反对巴勒斯坦加入联合国' '美媒称国际刑事法庭决定逮捕苏丹领导人'\n",
      " '阿西：无效的G20并非是坏事情' '查韦斯抨击美国解决洪都拉斯危机不力']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tabulate import tabulate\n",
    "from keras.optimizers import Adam\n",
    "from keras import Model, Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense, Dropout\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "seed_everything(seed=42)\n",
    "\n",
    "# Load training and testing dataframes\n",
    "train_df = pd.read_excel('train.xlsx')\n",
    "train_df.columns = ['text', 'class']\n",
    "\n",
    "test_df = pd.read_excel('test.xlsx')\n",
    "test_df.columns = ['text', 'class']\n",
    "\n",
    "# Split training set into 5 folds, stratified by class (only fold 0 is used in this notebook)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_df['fold'] = -1\n",
    "for i, (_, test_index) in enumerate(skf.split(train_df, train_df['class'])):\n",
    "    train_df.iloc[test_index, -1] = i\n",
    "table = pd.DataFrame(\n",
    "    {\n",
    "        f'fold{i}': train_df[train_df['fold'] == i].groupby('class').size()\n",
    "        for i in range(5)\n",
    "    }\n",
    "    | {'test': test_df.groupby('class').size()}\n",
    ")\n",
    "print(tabulate(table, headers='keys'))\n",
    "\n",
    "# Show random samples from the training set\n",
    "print(train_df['text'][train_df['class'] == 0].sample(5).values)\n",
    "print(train_df['text'][train_df['class'] == 1].sample(5).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "tokenizer: BertTokenizer = BertTokenizer.from_pretrained(bert_variant)\n",
    "train_tokens = tokenizer(\n",
    "    train_df['text'][train_df['fold'] != 0].tolist(),\n",
    "    return_tensors='tf',\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_length,\n",
    ")\n",
    "val_tokens = tokenizer(\n",
    "    train_df['text'][train_df['fold'] == 0].tolist(),\n",
    "    return_tensors='tf',\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_length,\n",
    ")\n",
    "test_tokens = tokenizer(\n",
    "    test_df['text'].tolist(),\n",
    "    return_tensors='tf',\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_length,\n",
    ")\n",
    "\n",
    "train_labels = train_df['class'][train_df['fold'] != 0].values.astype(np.int32)\n",
    "val_labels = train_df['class'][train_df['fold'] == 0].values.astype(np.int32)\n",
    "test_labels = test_df['class'].values.astype(np.int32)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_tokens), train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_tokens), val_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_tokens), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_classifier_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertModel)          multiple                  102267648 \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| bert (TFBertMainLayer)     multiple                  102267648|\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " head (Sequential)           (1, 2)                    1184258   \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| hidden (Dense)             (1, 1536)                 1181184  |\n",
      "|                                                               |\n",
      "| dropout (Dropout)          (1, 1536)                 0        |\n",
      "|                                                               |\n",
      "| output (Dense)             (1, 2)                    3074     |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "=================================================================\n",
      "Total params: 103451906 (394.64 MB)\n",
      "Trainable params: 103451906 (394.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Step 10 - loss: 0.5507 - val_accuracy: 0.9437 - val_f1: 0.9437\n",
      "Step 20 - loss: 0.3595 - val_accuracy: 0.9656 - val_f1: 0.9656\n",
      "Step 30 - loss: 0.2615 - val_accuracy: 0.9719 - val_f1: 0.9719\n",
      "Step 40 - loss: 0.2044 - val_accuracy: 0.9781 - val_f1: 0.9781\n",
      "Epoch 2/3\n",
      "Step 10 - loss: 0.0292 - val_accuracy: 0.9719 - val_f1: 0.9719\n",
      "Step 20 - loss: 0.0329 - val_accuracy: 0.9719 - val_f1: 0.9719\n",
      "Step 30 - loss: 0.0315 - val_accuracy: 0.9719 - val_f1: 0.9719\n",
      "Step 40 - loss: 0.0433 - val_accuracy: 0.9781 - val_f1: 0.9781\n",
      "Epoch 3/3\n",
      "Step 10 - loss: 0.0220 - val_accuracy: 0.9719 - val_f1: 0.9719\n",
      "Step 20 - loss: 0.0251 - val_accuracy: 0.9750 - val_f1: 0.9750\n",
      "Step 30 - loss: 0.0204 - val_accuracy: 0.9750 - val_f1: 0.9750\n",
      "Step 40 - loss: 0.0170 - val_accuracy: 0.9719 - val_f1: 0.9719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2980fc550>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 2e-5\n",
    "dropout = 0.1\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "class TextClassifier(Model):\n",
    "    def __init__(self, num_classes, mlp_ratio=2, dropout_rate=0.1, freeze_bert=False):\n",
    "        super().__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(bert_variant, name='bert')\n",
    "        self.bert.trainable = not freeze_bert\n",
    "        self.dropout = Dropout(dropout_rate, name='dropout')\n",
    "        embedding_size = self.bert.config.hidden_size\n",
    "        self.head = Sequential(\n",
    "            [\n",
    "                Dense(embedding_size * mlp_ratio, activation='silu', name='hidden'),\n",
    "                Dropout(dropout_rate, name='dropout'),\n",
    "                Dense(num_classes, activation='softmax', name='output'),\n",
    "            ],\n",
    "            name='head',\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        pooler_output = self.bert(inputs).pooler_output\n",
    "        pooler_output = self.dropout(pooler_output)\n",
    "        return self.head(pooler_output)\n",
    "\n",
    "\n",
    "class EvalCallback(Callback):\n",
    "    def __init__(\n",
    "        self, dataset, labels, prefix='val', output_file=f'{freeze_bert=}.csv'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.results = []\n",
    "        self.dataset = dataset\n",
    "        self.labels = labels\n",
    "        self.prefix = prefix\n",
    "        self.best_acc = 0\n",
    "        self.output_file = output_file\n",
    "        self.model: TextClassifier\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        loss = logs['loss']\n",
    "        print(f'\\rStep {batch + 1} - loss: {loss:.4f}', end='')\n",
    "        if (batch + 1) % 10 == 0:\n",
    "            y_pred = self.model.predict(self.dataset, verbose=0).argmax(axis=-1)\n",
    "            acc = accuracy_score(self.labels, y_pred)\n",
    "            f1 = f1_score(self.labels, y_pred, average='macro')\n",
    "            self.results.append([loss, acc, f1])\n",
    "            print(f' - {self.prefix}_accuracy: {acc:.4f} - {self.prefix}_f1: {f1:.4f}')\n",
    "            if acc > self.best_acc:\n",
    "                self.best_acc = acc\n",
    "                self.model.save_weights(f'{freeze_bert=}.h5')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pd.DataFrame(\n",
    "            self.results,\n",
    "            columns=[\n",
    "                f'{self.prefix}_loss',\n",
    "                f'{self.prefix}_accuracy',\n",
    "                f'{self.prefix}_f1',\n",
    "            ],\n",
    "        ).to_csv(self.output_file, index=False)\n",
    "\n",
    "\n",
    "seed_everything(seed=42)\n",
    "\n",
    "model = TextClassifier(num_classes=2, dropout_rate=dropout, freeze_bert=freeze_bert)\n",
    "model(train_tokens[:1])\n",
    "model.summary(expand_nested=True)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=lr), loss=SparseCategoricalCrossentropy())\n",
    "model.fit(\n",
    "    train_dataset.shuffle(1000).batch(batch_size),\n",
    "    epochs=num_epochs,\n",
    "    callbacks=[EvalCallback(val_dataset.batch(batch_size), val_labels)],\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 6s 438ms/step\n",
      "Test accuracy: 0.9875\n",
      "Test f1: 0.9875\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(f'{freeze_bert=}.h5')\n",
    "y_pred = model.predict(test_dataset.batch(batch_size)).argmax(axis=-1)\n",
    "print(f'Test accuracy: {accuracy_score(test_labels, y_pred):.4f}')\n",
    "print(f'Test f1: {f1_score(test_labels, y_pred, average=\"macro\"):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
