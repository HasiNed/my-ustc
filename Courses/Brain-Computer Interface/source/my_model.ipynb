{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bld\n",
      "data_dir:  ./data\n",
      "output_dir:  ./output/my_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3,4'\n",
    "data_dir = './data'\n",
    "output_dir = './output/my_model'\n",
    "\n",
    "print(platform.node())\n",
    "print('data_dir: ', data_dir)\n",
    "print('output_dir: ', output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import prepare_data, seed_everything\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, _ = prepare_data(data_dir)\n",
    "y_batch = next(iter(train_dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"spec_novel_cnn_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block0 (Sequential)         (256, 256, 32)            3232      \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 512, 32)            128      |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 512, 32)            3104     |\n",
      "|                                                               |\n",
      "| pool (AveragePooling1D)    (256, 256, 32)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " block1 (Sequential)         (256, 128, 64)            18560     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 256, 64)            6208     |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 256, 64)            12352    |\n",
      "|                                                               |\n",
      "| pool (AveragePooling1D)    (256, 128, 64)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " block2 (Sequential)         (256, 64, 128)            73984     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 128, 128)           24704    |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 128, 128)           49280    |\n",
      "|                                                               |\n",
      "| pool (AveragePooling1D)    (256, 64, 128)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " block3 (Sequential)         (256, 32, 256)            295424    \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 64, 256)            98560    |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 64, 256)            196864   |\n",
      "|                                                               |\n",
      "| dropout (Dropout)          (256, 64, 256)            0        |\n",
      "|                                                               |\n",
      "| pool (AveragePooling1D)    (256, 32, 256)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " block4 (Sequential)         (256, 16, 512)            1377280   \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 32, 512)            590336   |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 32, 512)            786944   |\n",
      "|                                                               |\n",
      "| dropout (Dropout)          (256, 32, 512)            0        |\n",
      "|                                                               |\n",
      "| pool (AveragePooling1D)    (256, 16, 512)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " block5 (Sequential)         (256, 8, 1024)            4917248   \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 16, 1024)           1770496  |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 16, 1024)           3146752  |\n",
      "|                                                               |\n",
      "| dropout (Dropout)          (256, 16, 1024)           0        |\n",
      "|                                                               |\n",
      "| pool (AveragePooling1D)    (256, 8, 1024)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " block6 (Sequential)         (256, 8, 2048)            19075072  \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 8, 2048)            6490112  |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 8, 2048)            12584960 |\n",
      "|                                                               |\n",
      "| dropout (Dropout)          (256, 8, 2048)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " spec_block0 (Sequential)    (256, 32, 32, 4)          296       \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv2D)             (256, 64, 64, 4)          148      |\n",
      "|                                                               |\n",
      "| conv2 (Conv2D)             (256, 64, 64, 4)          148      |\n",
      "|                                                               |\n",
      "| dropout (Dropout)          (256, 64, 64, 4)          0        |\n",
      "|                                                               |\n",
      "| pool (AveragePooling2D)    (256, 32, 32, 4)          0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " spec_block1 (Sequential)    (256, 16, 16, 4)          296       \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv2D)             (256, 32, 32, 4)          148      |\n",
      "|                                                               |\n",
      "| conv2 (Conv2D)             (256, 32, 32, 4)          148      |\n",
      "|                                                               |\n",
      "| dropout (Dropout)          (256, 32, 32, 4)          0        |\n",
      "|                                                               |\n",
      "| pool (AveragePooling2D)    (256, 16, 16, 4)          0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " spec_block2 (Sequential)    (256, 8, 8, 4)            296       \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv2D)             (256, 16, 16, 4)          148      |\n",
      "|                                                               |\n",
      "| conv2 (Conv2D)             (256, 16, 16, 4)          148      |\n",
      "|                                                               |\n",
      "| dropout (Dropout)          (256, 16, 16, 4)          0        |\n",
      "|                                                               |\n",
      "| pool (AveragePooling2D)    (256, 8, 8, 4)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " fc (Dense)                  multiple                  8389120   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34150808 (130.27 MB)\n",
      "Trainable params: 34150808 (130.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models import SpecNovelCNN\n",
    "\n",
    "seed_everything(0)\n",
    "\n",
    "model = SpecNovelCNN()\n",
    "model(y_batch)  # build the model by passing a sample input\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 07:24:02.788591: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inspec_novel_cnn_1/spec_block0/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 14s 37ms/step - loss: 0.5975 - cc: 0.0684 - rrmse: 0.9980 - snr: 0.0182 - val_loss: 0.5036 - val_cc: 0.0817 - val_rrmse: 0.9975 - val_snr: 0.0222\n",
      "Epoch 2/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.4944 - cc: 0.2168 - rrmse: 0.9745 - snr: 0.2304 - val_loss: 0.4696 - val_cc: 0.2711 - val_rrmse: 0.9626 - val_snr: 0.3370\n",
      "Epoch 3/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.4573 - cc: 0.3175 - rrmse: 0.9487 - snr: 0.4683 - val_loss: 0.4411 - val_cc: 0.3659 - val_rrmse: 0.9321 - val_snr: 0.6204\n",
      "Epoch 4/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.4380 - cc: 0.3807 - rrmse: 0.9276 - snr: 0.6695 - val_loss: 0.4240 - val_cc: 0.4062 - val_rrmse: 0.9136 - val_snr: 0.7999\n",
      "Epoch 5/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.4239 - cc: 0.3948 - rrmse: 0.9196 - snr: 0.7482 - val_loss: 0.4096 - val_cc: 0.4340 - val_rrmse: 0.8992 - val_snr: 0.9443\n",
      "Epoch 6/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.4122 - cc: 0.4468 - rrmse: 0.8934 - snr: 1.0079 - val_loss: 0.3951 - val_cc: 0.4623 - val_rrmse: 0.8821 - val_snr: 1.1281\n",
      "Epoch 7/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.3962 - cc: 0.4719 - rrmse: 0.8768 - snr: 1.1864 - val_loss: 0.3683 - val_cc: 0.5111 - val_rrmse: 0.8499 - val_snr: 1.4783\n",
      "Epoch 8/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.3506 - cc: 0.5985 - rrmse: 0.7990 - snr: 2.0098 - val_loss: 0.2768 - val_cc: 0.6672 - val_rrmse: 0.7355 - val_snr: 2.7699\n",
      "Epoch 9/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.2978 - cc: 0.6444 - rrmse: 0.7611 - snr: 2.4465 - val_loss: 0.2433 - val_cc: 0.7134 - val_rrmse: 0.6893 - val_snr: 3.3687\n",
      "Epoch 10/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.2717 - cc: 0.6888 - rrmse: 0.7202 - snr: 2.9521 - val_loss: 0.2232 - val_cc: 0.7405 - val_rrmse: 0.6579 - val_snr: 3.8239\n",
      "Epoch 11/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.2527 - cc: 0.7105 - rrmse: 0.6926 - snr: 3.3442 - val_loss: 0.2057 - val_cc: 0.7632 - val_rrmse: 0.6287 - val_snr: 4.2759\n",
      "Epoch 12/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.2370 - cc: 0.7361 - rrmse: 0.6692 - snr: 3.6471 - val_loss: 0.1913 - val_cc: 0.7833 - val_rrmse: 0.6025 - val_snr: 4.6855\n",
      "Epoch 13/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.2236 - cc: 0.7399 - rrmse: 0.6638 - snr: 3.7286 - val_loss: 0.1795 - val_cc: 0.7977 - val_rrmse: 0.5819 - val_snr: 5.0382\n",
      "Epoch 14/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.2125 - cc: 0.7639 - rrmse: 0.6382 - snr: 4.0854 - val_loss: 0.1701 - val_cc: 0.8105 - val_rrmse: 0.5644 - val_snr: 5.3401\n",
      "Epoch 15/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.2017 - cc: 0.7753 - rrmse: 0.6253 - snr: 4.2522 - val_loss: 0.1598 - val_cc: 0.8211 - val_rrmse: 0.5445 - val_snr: 5.7221\n",
      "Epoch 16/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1926 - cc: 0.7802 - rrmse: 0.6175 - snr: 4.3833 - val_loss: 0.1537 - val_cc: 0.8293 - val_rrmse: 0.5332 - val_snr: 5.9263\n",
      "Epoch 17/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.1848 - cc: 0.7959 - rrmse: 0.5965 - snr: 4.7149 - val_loss: 0.1466 - val_cc: 0.8371 - val_rrmse: 0.5187 - val_snr: 6.2130\n",
      "Epoch 18/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.1762 - cc: 0.7978 - rrmse: 0.5902 - snr: 4.8326 - val_loss: 0.1408 - val_cc: 0.8432 - val_rrmse: 0.5094 - val_snr: 6.3897\n",
      "Epoch 19/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.1689 - cc: 0.8187 - rrmse: 0.5630 - snr: 5.2610 - val_loss: 0.1357 - val_cc: 0.8491 - val_rrmse: 0.4999 - val_snr: 6.5738\n",
      "Epoch 20/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.1638 - cc: 0.8235 - rrmse: 0.5597 - snr: 5.2928 - val_loss: 0.1312 - val_cc: 0.8538 - val_rrmse: 0.4886 - val_snr: 6.8372\n",
      "Epoch 21/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.1582 - cc: 0.8413 - rrmse: 0.5323 - snr: 5.7619 - val_loss: 0.1284 - val_cc: 0.8586 - val_rrmse: 0.4863 - val_snr: 6.8217\n",
      "Epoch 22/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.1528 - cc: 0.8336 - rrmse: 0.5446 - snr: 5.5386 - val_loss: 0.1241 - val_cc: 0.8622 - val_rrmse: 0.4768 - val_snr: 7.0440\n",
      "Epoch 23/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.1480 - cc: 0.8336 - rrmse: 0.5431 - snr: 5.5723 - val_loss: 0.1217 - val_cc: 0.8659 - val_rrmse: 0.4728 - val_snr: 7.1046\n",
      "Epoch 24/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1430 - cc: 0.8467 - rrmse: 0.5247 - snr: 5.8537 - val_loss: 0.1179 - val_cc: 0.8694 - val_rrmse: 0.4636 - val_snr: 7.3169\n",
      "Epoch 25/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1392 - cc: 0.8559 - rrmse: 0.5117 - snr: 6.1217 - val_loss: 0.1155 - val_cc: 0.8725 - val_rrmse: 0.4607 - val_snr: 7.3538\n",
      "Epoch 26/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1348 - cc: 0.8602 - rrmse: 0.5050 - snr: 6.2138 - val_loss: 0.1121 - val_cc: 0.8754 - val_rrmse: 0.4523 - val_snr: 7.5658\n",
      "Epoch 27/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.1298 - cc: 0.8572 - rrmse: 0.5094 - snr: 6.1165 - val_loss: 0.1101 - val_cc: 0.8778 - val_rrmse: 0.4488 - val_snr: 7.6281\n",
      "Epoch 28/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.1267 - cc: 0.8729 - rrmse: 0.4864 - snr: 6.5437 - val_loss: 0.1077 - val_cc: 0.8806 - val_rrmse: 0.4432 - val_snr: 7.7531\n",
      "Epoch 29/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1229 - cc: 0.8721 - rrmse: 0.4872 - snr: 6.4945 - val_loss: 0.1044 - val_cc: 0.8833 - val_rrmse: 0.4356 - val_snr: 7.9527\n",
      "Epoch 30/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.1194 - cc: 0.8791 - rrmse: 0.4770 - snr: 6.6871 - val_loss: 0.1036 - val_cc: 0.8843 - val_rrmse: 0.4376 - val_snr: 7.8593\n",
      "Epoch 31/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1157 - cc: 0.8805 - rrmse: 0.4768 - snr: 6.7112 - val_loss: 0.1008 - val_cc: 0.8868 - val_rrmse: 0.4288 - val_snr: 8.1040\n",
      "Epoch 32/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.1127 - cc: 0.8810 - rrmse: 0.4730 - snr: 6.7795 - val_loss: 0.0991 - val_cc: 0.8882 - val_rrmse: 0.4238 - val_snr: 8.2545\n",
      "Epoch 33/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.1091 - cc: 0.8816 - rrmse: 0.4697 - snr: 6.8380 - val_loss: 0.0987 - val_cc: 0.8900 - val_rrmse: 0.4263 - val_snr: 8.1192\n",
      "Epoch 34/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.1063 - cc: 0.8918 - rrmse: 0.4522 - snr: 7.1899 - val_loss: 0.0958 - val_cc: 0.8917 - val_rrmse: 0.4153 - val_snr: 8.4859\n",
      "Epoch 35/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1031 - cc: 0.8997 - rrmse: 0.4423 - snr: 7.3611 - val_loss: 0.0944 - val_cc: 0.8936 - val_rrmse: 0.4118 - val_snr: 8.5653\n",
      "Epoch 36/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.1004 - cc: 0.8989 - rrmse: 0.4422 - snr: 7.3604 - val_loss: 0.0931 - val_cc: 0.8950 - val_rrmse: 0.4105 - val_snr: 8.5601\n",
      "Epoch 37/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0977 - cc: 0.9054 - rrmse: 0.4284 - snr: 7.6295 - val_loss: 0.0916 - val_cc: 0.8969 - val_rrmse: 0.4080 - val_snr: 8.5993\n",
      "Epoch 38/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0947 - cc: 0.9082 - rrmse: 0.4255 - snr: 7.6939 - val_loss: 0.0899 - val_cc: 0.8987 - val_rrmse: 0.4025 - val_snr: 8.7478\n",
      "Epoch 39/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0921 - cc: 0.9059 - rrmse: 0.4293 - snr: 7.6389 - val_loss: 0.0892 - val_cc: 0.8994 - val_rrmse: 0.4021 - val_snr: 8.7539\n",
      "Epoch 40/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0900 - cc: 0.9097 - rrmse: 0.4216 - snr: 7.7871 - val_loss: 0.0880 - val_cc: 0.9007 - val_rrmse: 0.3978 - val_snr: 8.8726\n",
      "Epoch 41/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0881 - cc: 0.9143 - rrmse: 0.4088 - snr: 8.0604 - val_loss: 0.0876 - val_cc: 0.9018 - val_rrmse: 0.3995 - val_snr: 8.7785\n",
      "Epoch 42/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0854 - cc: 0.9150 - rrmse: 0.4097 - snr: 8.0335 - val_loss: 0.0864 - val_cc: 0.9030 - val_rrmse: 0.3948 - val_snr: 8.9296\n",
      "Epoch 43/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0839 - cc: 0.9185 - rrmse: 0.4007 - snr: 8.2324 - val_loss: 0.0854 - val_cc: 0.9033 - val_rrmse: 0.3897 - val_snr: 9.1677\n",
      "Epoch 44/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0821 - cc: 0.9208 - rrmse: 0.4009 - snr: 8.2127 - val_loss: 0.0847 - val_cc: 0.9041 - val_rrmse: 0.3899 - val_snr: 9.0907\n",
      "Epoch 45/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0799 - cc: 0.9283 - rrmse: 0.3819 - snr: 8.6679 - val_loss: 0.0836 - val_cc: 0.9057 - val_rrmse: 0.3847 - val_snr: 9.3120\n",
      "Epoch 46/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0785 - cc: 0.9260 - rrmse: 0.3898 - snr: 8.5026 - val_loss: 0.0828 - val_cc: 0.9067 - val_rrmse: 0.3843 - val_snr: 9.2680\n",
      "Epoch 47/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0768 - cc: 0.9276 - rrmse: 0.3857 - snr: 8.5719 - val_loss: 0.0824 - val_cc: 0.9071 - val_rrmse: 0.3809 - val_snr: 9.4035\n",
      "Epoch 48/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0747 - cc: 0.9296 - rrmse: 0.3774 - snr: 8.7729 - val_loss: 0.0817 - val_cc: 0.9083 - val_rrmse: 0.3789 - val_snr: 9.4700\n",
      "Epoch 49/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0734 - cc: 0.9345 - rrmse: 0.3683 - snr: 8.9501 - val_loss: 0.0812 - val_cc: 0.9084 - val_rrmse: 0.3796 - val_snr: 9.3962\n",
      "Epoch 50/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0721 - cc: 0.9371 - rrmse: 0.3645 - snr: 9.0287 - val_loss: 0.0807 - val_cc: 0.9095 - val_rrmse: 0.3786 - val_snr: 9.4032\n",
      "Epoch 51/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0708 - cc: 0.9301 - rrmse: 0.3777 - snr: 8.7916 - val_loss: 0.0803 - val_cc: 0.9101 - val_rrmse: 0.3746 - val_snr: 9.5686\n",
      "Epoch 52/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0693 - cc: 0.9356 - rrmse: 0.3693 - snr: 8.9273 - val_loss: 0.0796 - val_cc: 0.9107 - val_rrmse: 0.3717 - val_snr: 9.6751\n",
      "Epoch 53/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0681 - cc: 0.9349 - rrmse: 0.3675 - snr: 9.0519 - val_loss: 0.0798 - val_cc: 0.9113 - val_rrmse: 0.3717 - val_snr: 9.7087\n",
      "Epoch 54/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0668 - cc: 0.9374 - rrmse: 0.3606 - snr: 9.1677 - val_loss: 0.0795 - val_cc: 0.9114 - val_rrmse: 0.3696 - val_snr: 9.7955\n",
      "Epoch 55/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0658 - cc: 0.9351 - rrmse: 0.3641 - snr: 9.1393 - val_loss: 0.0790 - val_cc: 0.9117 - val_rrmse: 0.3697 - val_snr: 9.7567\n",
      "Epoch 56/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0648 - cc: 0.9431 - rrmse: 0.3504 - snr: 9.3994 - val_loss: 0.0789 - val_cc: 0.9125 - val_rrmse: 0.3692 - val_snr: 9.7868\n",
      "Epoch 57/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0639 - cc: 0.9428 - rrmse: 0.3466 - snr: 9.4778 - val_loss: 0.0790 - val_cc: 0.9122 - val_rrmse: 0.3693 - val_snr: 9.7932\n",
      "Epoch 58/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0631 - cc: 0.9433 - rrmse: 0.3519 - snr: 9.4163 - val_loss: 0.0784 - val_cc: 0.9129 - val_rrmse: 0.3666 - val_snr: 9.8981\n",
      "Epoch 59/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0615 - cc: 0.9458 - rrmse: 0.3410 - snr: 9.6693 - val_loss: 0.0781 - val_cc: 0.9140 - val_rrmse: 0.3653 - val_snr: 9.9183\n",
      "Epoch 60/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0609 - cc: 0.9457 - rrmse: 0.3437 - snr: 9.5640 - val_loss: 0.0777 - val_cc: 0.9139 - val_rrmse: 0.3649 - val_snr: 9.9298\n",
      "Epoch 61/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0600 - cc: 0.9470 - rrmse: 0.3364 - snr: 9.7938 - val_loss: 0.0791 - val_cc: 0.9139 - val_rrmse: 0.3652 - val_snr: 10.0144\n",
      "Epoch 62/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0592 - cc: 0.9405 - rrmse: 0.3488 - snr: 9.5174 - val_loss: 0.0773 - val_cc: 0.9145 - val_rrmse: 0.3625 - val_snr: 10.0414\n",
      "Epoch 63/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0583 - cc: 0.9466 - rrmse: 0.3314 - snr: 9.9475 - val_loss: 0.0769 - val_cc: 0.9150 - val_rrmse: 0.3609 - val_snr: 10.0771\n",
      "Epoch 64/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0579 - cc: 0.9457 - rrmse: 0.3403 - snr: 9.6948 - val_loss: 0.0769 - val_cc: 0.9159 - val_rrmse: 0.3598 - val_snr: 10.1103\n",
      "Epoch 65/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0569 - cc: 0.9455 - rrmse: 0.3439 - snr: 9.6595 - val_loss: 0.0778 - val_cc: 0.9158 - val_rrmse: 0.3611 - val_snr: 10.1428\n",
      "Epoch 66/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0559 - cc: 0.9473 - rrmse: 0.3273 - snr: 10.0827 - val_loss: 0.0779 - val_cc: 0.9154 - val_rrmse: 0.3598 - val_snr: 10.1902\n",
      "Epoch 67/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0552 - cc: 0.9497 - rrmse: 0.3301 - snr: 10.0189 - val_loss: 0.0769 - val_cc: 0.9165 - val_rrmse: 0.3585 - val_snr: 10.1910\n",
      "Epoch 68/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0548 - cc: 0.9494 - rrmse: 0.3309 - snr: 9.9837 - val_loss: 0.0770 - val_cc: 0.9162 - val_rrmse: 0.3584 - val_snr: 10.1935\n",
      "Epoch 69/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0540 - cc: 0.9547 - rrmse: 0.3175 - snr: 10.3001 - val_loss: 0.0766 - val_cc: 0.9167 - val_rrmse: 0.3574 - val_snr: 10.2037\n",
      "Epoch 70/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0539 - cc: 0.9498 - rrmse: 0.3287 - snr: 10.0393 - val_loss: 0.0767 - val_cc: 0.9166 - val_rrmse: 0.3575 - val_snr: 10.2200\n",
      "Epoch 71/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0532 - cc: 0.9550 - rrmse: 0.3145 - snr: 10.3990 - val_loss: 0.0772 - val_cc: 0.9165 - val_rrmse: 0.3583 - val_snr: 10.2366\n",
      "Epoch 72/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0526 - cc: 0.9570 - rrmse: 0.3121 - snr: 10.4550 - val_loss: 0.0765 - val_cc: 0.9171 - val_rrmse: 0.3568 - val_snr: 10.2786\n",
      "Epoch 73/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0523 - cc: 0.9510 - rrmse: 0.3208 - snr: 10.3289 - val_loss: 0.0768 - val_cc: 0.9173 - val_rrmse: 0.3562 - val_snr: 10.3144\n",
      "Epoch 74/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0516 - cc: 0.9563 - rrmse: 0.3090 - snr: 10.5361 - val_loss: 0.0760 - val_cc: 0.9178 - val_rrmse: 0.3542 - val_snr: 10.3453\n",
      "Epoch 75/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0510 - cc: 0.9560 - rrmse: 0.3156 - snr: 10.3996 - val_loss: 0.0764 - val_cc: 0.9173 - val_rrmse: 0.3552 - val_snr: 10.3285\n",
      "Epoch 76/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0506 - cc: 0.9564 - rrmse: 0.3097 - snr: 10.5415 - val_loss: 0.0768 - val_cc: 0.9179 - val_rrmse: 0.3558 - val_snr: 10.3455\n",
      "Epoch 77/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0503 - cc: 0.9529 - rrmse: 0.3193 - snr: 10.2937 - val_loss: 0.0764 - val_cc: 0.9178 - val_rrmse: 0.3554 - val_snr: 10.3312\n",
      "Epoch 78/200\n",
      "200/200 [==============================] - 7s 37ms/step - loss: 0.0498 - cc: 0.9575 - rrmse: 0.3107 - snr: 10.5269 - val_loss: 0.0764 - val_cc: 0.9187 - val_rrmse: 0.3538 - val_snr: 10.3979\n",
      "Epoch 79/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0495 - cc: 0.9593 - rrmse: 0.3037 - snr: 10.6654 - val_loss: 0.0763 - val_cc: 0.9184 - val_rrmse: 0.3535 - val_snr: 10.4149\n",
      "Epoch 80/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0488 - cc: 0.9626 - rrmse: 0.2988 - snr: 10.7725 - val_loss: 0.0754 - val_cc: 0.9185 - val_rrmse: 0.3525 - val_snr: 10.4039\n",
      "Epoch 81/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0484 - cc: 0.9553 - rrmse: 0.3115 - snr: 10.5173 - val_loss: 0.0765 - val_cc: 0.9188 - val_rrmse: 0.3535 - val_snr: 10.4358\n",
      "Epoch 82/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0479 - cc: 0.9541 - rrmse: 0.3235 - snr: 10.1480 - val_loss: 0.0757 - val_cc: 0.9190 - val_rrmse: 0.3522 - val_snr: 10.4409\n",
      "Epoch 83/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0478 - cc: 0.9594 - rrmse: 0.3011 - snr: 10.7921 - val_loss: 0.0770 - val_cc: 0.9192 - val_rrmse: 0.3544 - val_snr: 10.4106\n",
      "Epoch 84/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0474 - cc: 0.9569 - rrmse: 0.3129 - snr: 10.4339 - val_loss: 0.0756 - val_cc: 0.9192 - val_rrmse: 0.3515 - val_snr: 10.4734\n",
      "Epoch 85/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0472 - cc: 0.9627 - rrmse: 0.2925 - snr: 10.9944 - val_loss: 0.0767 - val_cc: 0.9190 - val_rrmse: 0.3535 - val_snr: 10.4493\n",
      "Epoch 86/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0467 - cc: 0.9629 - rrmse: 0.2938 - snr: 10.9835 - val_loss: 0.0765 - val_cc: 0.9196 - val_rrmse: 0.3532 - val_snr: 10.4739\n",
      "Epoch 87/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0461 - cc: 0.9607 - rrmse: 0.3002 - snr: 10.8287 - val_loss: 0.0763 - val_cc: 0.9192 - val_rrmse: 0.3527 - val_snr: 10.4762\n",
      "Epoch 88/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0461 - cc: 0.9610 - rrmse: 0.2961 - snr: 10.9495 - val_loss: 0.0762 - val_cc: 0.9199 - val_rrmse: 0.3521 - val_snr: 10.4833\n",
      "Epoch 89/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0459 - cc: 0.9643 - rrmse: 0.2929 - snr: 11.0109 - val_loss: 0.0770 - val_cc: 0.9194 - val_rrmse: 0.3533 - val_snr: 10.4697\n",
      "Epoch 90/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0457 - cc: 0.9621 - rrmse: 0.2983 - snr: 10.8352 - val_loss: 0.0766 - val_cc: 0.9201 - val_rrmse: 0.3522 - val_snr: 10.5181\n",
      "Epoch 91/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0450 - cc: 0.9621 - rrmse: 0.2985 - snr: 10.8933 - val_loss: 0.0778 - val_cc: 0.9199 - val_rrmse: 0.3549 - val_snr: 10.4104\n",
      "Epoch 92/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0448 - cc: 0.9627 - rrmse: 0.2943 - snr: 11.0467 - val_loss: 0.0777 - val_cc: 0.9209 - val_rrmse: 0.3543 - val_snr: 10.4653\n",
      "Epoch 93/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0446 - cc: 0.9610 - rrmse: 0.2948 - snr: 11.0043 - val_loss: 0.0764 - val_cc: 0.9207 - val_rrmse: 0.3515 - val_snr: 10.5207\n",
      "Epoch 94/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0444 - cc: 0.9628 - rrmse: 0.2938 - snr: 11.0341 - val_loss: 0.0781 - val_cc: 0.9202 - val_rrmse: 0.3550 - val_snr: 10.4481\n",
      "Epoch 95/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0441 - cc: 0.9606 - rrmse: 0.2998 - snr: 10.8466 - val_loss: 0.0781 - val_cc: 0.9201 - val_rrmse: 0.3566 - val_snr: 10.3813\n",
      "Epoch 96/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0438 - cc: 0.9623 - rrmse: 0.2937 - snr: 11.0765 - val_loss: 0.0767 - val_cc: 0.9206 - val_rrmse: 0.3518 - val_snr: 10.5395\n",
      "Epoch 97/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0436 - cc: 0.9618 - rrmse: 0.2935 - snr: 11.0132 - val_loss: 0.0774 - val_cc: 0.9208 - val_rrmse: 0.3542 - val_snr: 10.4757\n",
      "Epoch 98/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0433 - cc: 0.9625 - rrmse: 0.2877 - snr: 11.2003 - val_loss: 0.0756 - val_cc: 0.9215 - val_rrmse: 0.3484 - val_snr: 10.5980\n",
      "Epoch 99/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0432 - cc: 0.9633 - rrmse: 0.2924 - snr: 11.0557 - val_loss: 0.0787 - val_cc: 0.9208 - val_rrmse: 0.3556 - val_snr: 10.4633\n",
      "Epoch 100/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0430 - cc: 0.9622 - rrmse: 0.2923 - snr: 11.0874 - val_loss: 0.0778 - val_cc: 0.9212 - val_rrmse: 0.3535 - val_snr: 10.5223\n",
      "Epoch 101/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0430 - cc: 0.9629 - rrmse: 0.2916 - snr: 11.0862 - val_loss: 0.0781 - val_cc: 0.9209 - val_rrmse: 0.3535 - val_snr: 10.5278\n",
      "Epoch 102/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0425 - cc: 0.9661 - rrmse: 0.2832 - snr: 11.3353 - val_loss: 0.0779 - val_cc: 0.9216 - val_rrmse: 0.3532 - val_snr: 10.5192\n",
      "Epoch 103/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0423 - cc: 0.9627 - rrmse: 0.2898 - snr: 11.1544 - val_loss: 0.0781 - val_cc: 0.9210 - val_rrmse: 0.3547 - val_snr: 10.4540\n",
      "Epoch 104/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0420 - cc: 0.9627 - rrmse: 0.2896 - snr: 11.2113 - val_loss: 0.0774 - val_cc: 0.9215 - val_rrmse: 0.3517 - val_snr: 10.5459\n",
      "Epoch 105/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0418 - cc: 0.9658 - rrmse: 0.2780 - snr: 11.5085 - val_loss: 0.0770 - val_cc: 0.9218 - val_rrmse: 0.3504 - val_snr: 10.5999\n",
      "Epoch 106/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0419 - cc: 0.9663 - rrmse: 0.2839 - snr: 11.3568 - val_loss: 0.0785 - val_cc: 0.9213 - val_rrmse: 0.3542 - val_snr: 10.5120\n",
      "Epoch 107/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0415 - cc: 0.9673 - rrmse: 0.2764 - snr: 11.5096 - val_loss: 0.0772 - val_cc: 0.9216 - val_rrmse: 0.3520 - val_snr: 10.5647\n",
      "Epoch 108/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0415 - cc: 0.9701 - rrmse: 0.2707 - snr: 11.7054 - val_loss: 0.0781 - val_cc: 0.9216 - val_rrmse: 0.3534 - val_snr: 10.5460\n",
      "Epoch 109/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0410 - cc: 0.9662 - rrmse: 0.2835 - snr: 11.3306 - val_loss: 0.0790 - val_cc: 0.9215 - val_rrmse: 0.3552 - val_snr: 10.4802\n",
      "Epoch 110/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0409 - cc: 0.9672 - rrmse: 0.2786 - snr: 11.4655 - val_loss: 0.0787 - val_cc: 0.9218 - val_rrmse: 0.3554 - val_snr: 10.4779\n",
      "Epoch 111/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0406 - cc: 0.9687 - rrmse: 0.2714 - snr: 11.6995 - val_loss: 0.0775 - val_cc: 0.9215 - val_rrmse: 0.3524 - val_snr: 10.5666\n",
      "Epoch 112/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0407 - cc: 0.9643 - rrmse: 0.2843 - snr: 11.3072 - val_loss: 0.0789 - val_cc: 0.9219 - val_rrmse: 0.3561 - val_snr: 10.4658\n",
      "Epoch 113/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0403 - cc: 0.9683 - rrmse: 0.2753 - snr: 11.6181 - val_loss: 0.0796 - val_cc: 0.9215 - val_rrmse: 0.3579 - val_snr: 10.4172\n",
      "Epoch 114/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0401 - cc: 0.9686 - rrmse: 0.2759 - snr: 11.5914 - val_loss: 0.0784 - val_cc: 0.9221 - val_rrmse: 0.3539 - val_snr: 10.5227\n",
      "Epoch 115/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0398 - cc: 0.9656 - rrmse: 0.2804 - snr: 11.4207 - val_loss: 0.0782 - val_cc: 0.9221 - val_rrmse: 0.3533 - val_snr: 10.5585\n",
      "Epoch 116/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0399 - cc: 0.9679 - rrmse: 0.2768 - snr: 11.5249 - val_loss: 0.0771 - val_cc: 0.9221 - val_rrmse: 0.3516 - val_snr: 10.5987\n",
      "Epoch 117/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0399 - cc: 0.9668 - rrmse: 0.2825 - snr: 11.4046 - val_loss: 0.0801 - val_cc: 0.9216 - val_rrmse: 0.3588 - val_snr: 10.4000\n",
      "Epoch 118/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0396 - cc: 0.9694 - rrmse: 0.2698 - snr: 11.7561 - val_loss: 0.0785 - val_cc: 0.9221 - val_rrmse: 0.3549 - val_snr: 10.5112\n",
      "Epoch 119/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0397 - cc: 0.9660 - rrmse: 0.2800 - snr: 11.4440 - val_loss: 0.0777 - val_cc: 0.9225 - val_rrmse: 0.3516 - val_snr: 10.5886\n",
      "Epoch 120/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0393 - cc: 0.9669 - rrmse: 0.2786 - snr: 11.4880 - val_loss: 0.0793 - val_cc: 0.9223 - val_rrmse: 0.3554 - val_snr: 10.4845\n",
      "Epoch 121/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0389 - cc: 0.9685 - rrmse: 0.2704 - snr: 11.7208 - val_loss: 0.0778 - val_cc: 0.9222 - val_rrmse: 0.3524 - val_snr: 10.5608\n",
      "Epoch 122/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0389 - cc: 0.9689 - rrmse: 0.2726 - snr: 11.7064 - val_loss: 0.0796 - val_cc: 0.9223 - val_rrmse: 0.3573 - val_snr: 10.4230\n",
      "Epoch 123/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0388 - cc: 0.9680 - rrmse: 0.2721 - snr: 11.7266 - val_loss: 0.0794 - val_cc: 0.9230 - val_rrmse: 0.3557 - val_snr: 10.4480\n",
      "Epoch 124/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0389 - cc: 0.9676 - rrmse: 0.2755 - snr: 11.5854 - val_loss: 0.0789 - val_cc: 0.9230 - val_rrmse: 0.3540 - val_snr: 10.5217\n",
      "Epoch 125/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0384 - cc: 0.9698 - rrmse: 0.2679 - snr: 11.8711Restoring model weights from the end of the best epoch: 105.\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0385 - cc: 0.9675 - rrmse: 0.2771 - snr: 11.6080 - val_loss: 0.0801 - val_cc: 0.9226 - val_rrmse: 0.3582 - val_snr: 10.3590\n",
      "Epoch 125: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, CSVLogger\n",
    "\n",
    "from metrics import CC, RRMSE, SNR\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=5e-5, epsilon=1e-8),\n",
    "    loss=MeanSquaredError(),\n",
    "    metrics=[CC(), RRMSE(), SNR()],\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_snr',\n",
    "        mode='max',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'{output_dir}/my_model.h5',\n",
    "        mode='max',\n",
    "        monitor='val_snr',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    ),\n",
    "    TensorBoard(log_dir=output_dir),\n",
    "    CSVLogger(f'{output_dir}/my_model.csv'),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=200,\n",
    "    steps_per_epoch=200,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0794 - cc: 0.9235 - rrmse: 0.3382 - snr: 10.9835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07940466701984406,\n",
       " 0.9234558343887329,\n",
       " 0.3382062315940857,\n",
       " 10.983481407165527]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
