{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bld\n",
      "data_dir:  ./data\n",
      "output_dir:  ./output/novel_cnn\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,3'\n",
    "data_dir = './data'\n",
    "output_dir = './output/novel_cnn'\n",
    "\n",
    "print(platform.node())\n",
    "print('data_dir: ', data_dir)\n",
    "print('output_dir: ', output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import prepare_data, seed_everything\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, _ = prepare_data(data_dir)\n",
    "y_batch = next(iter(train_dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"novel_cnn_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block0 (Sequential)         (256, 256, 32)            3232      \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 512, 32)            128      |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 512, 32)            3104     |\n",
      "|                                                               |\n",
      "| pool (AveragePooling1D)    (256, 256, 32)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " block1 (Sequential)         (256, 128, 64)            18560     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 256, 64)            6208     |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 256, 64)            12352    |\n",
      "|                                                               |\n",
      "| pool (AveragePooling1D)    (256, 128, 64)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " block2 (Sequential)         (256, 64, 128)            73984     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 128, 128)           24704    |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 128, 128)           49280    |\n",
      "|                                                               |\n",
      "| pool (AveragePooling1D)    (256, 64, 128)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " block3 (Sequential)         (256, 32, 256)            295424    \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 64, 256)            98560    |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 64, 256)            196864   |\n",
      "|                                                               |\n",
      "| dropout (Dropout)          (256, 64, 256)            0        |\n",
      "|                                                               |\n",
      "| pool (AveragePooling1D)    (256, 32, 256)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " block4 (Sequential)         (256, 16, 512)            1180672   \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 32, 512)            393728   |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 32, 512)            786944   |\n",
      "|                                                               |\n",
      "| dropout (Dropout)          (256, 32, 512)            0        |\n",
      "|                                                               |\n",
      "| pool (AveragePooling1D)    (256, 16, 512)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " block5 (Sequential)         (256, 8, 1024)            4720640   \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 16, 1024)           1573888  |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 16, 1024)           3146752  |\n",
      "|                                                               |\n",
      "| dropout (Dropout)          (256, 16, 1024)           0        |\n",
      "|                                                               |\n",
      "| pool (AveragePooling1D)    (256, 8, 1024)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " block6 (Sequential)         (256, 8, 2048)            18878464  \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv1 (Conv1D)             (256, 8, 2048)            6293504  |\n",
      "|                                                               |\n",
      "| conv2 (Conv1D)             (256, 8, 2048)            12584960 |\n",
      "|                                                               |\n",
      "| dropout (Dropout)          (256, 8, 2048)            0        |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " fc (Dense)                  multiple                  8389120   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33560096 (128.02 MB)\n",
      "Trainable params: 33560096 (128.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models import NovelCNN\n",
    "\n",
    "seed_everything(0)\n",
    "\n",
    "model = NovelCNN()\n",
    "model(y_batch)  # build the model by passing a sample input\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "200/200 [==============================] - 12s 36ms/step - loss: 0.5963 - cc: 0.0625 - rrmse: 0.9983 - snr: 0.0150 - val_loss: 0.5040 - val_cc: 0.0810 - val_rrmse: 0.9978 - val_snr: 0.0190\n",
      "Epoch 2/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.4927 - cc: 0.2274 - rrmse: 0.9747 - snr: 0.2297 - val_loss: 0.4678 - val_cc: 0.2704 - val_rrmse: 0.9611 - val_snr: 0.3523\n",
      "Epoch 3/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.4579 - cc: 0.3146 - rrmse: 0.9508 - snr: 0.4499 - val_loss: 0.4415 - val_cc: 0.3633 - val_rrmse: 0.9326 - val_snr: 0.6168\n",
      "Epoch 4/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.4392 - cc: 0.3752 - rrmse: 0.9280 - snr: 0.6639 - val_loss: 0.4250 - val_cc: 0.4019 - val_rrmse: 0.9156 - val_snr: 0.7813\n",
      "Epoch 5/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.4258 - cc: 0.3910 - rrmse: 0.9210 - snr: 0.7330 - val_loss: 0.4110 - val_cc: 0.4291 - val_rrmse: 0.9015 - val_snr: 0.9206\n",
      "Epoch 6/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.4147 - cc: 0.4372 - rrmse: 0.9001 - snr: 0.9384 - val_loss: 0.3974 - val_cc: 0.4556 - val_rrmse: 0.8863 - val_snr: 1.0815\n",
      "Epoch 7/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.3977 - cc: 0.4821 - rrmse: 0.8718 - snr: 1.2336 - val_loss: 0.3579 - val_cc: 0.5345 - val_rrmse: 0.8370 - val_snr: 1.6079\n",
      "Epoch 8/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.3386 - cc: 0.6088 - rrmse: 0.7920 - snr: 2.0824 - val_loss: 0.2709 - val_cc: 0.6744 - val_rrmse: 0.7298 - val_snr: 2.8322\n",
      "Epoch 9/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.2956 - cc: 0.6507 - rrmse: 0.7559 - snr: 2.5108 - val_loss: 0.2428 - val_cc: 0.7121 - val_rrmse: 0.6906 - val_snr: 3.3561\n",
      "Epoch 10/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.2724 - cc: 0.6859 - rrmse: 0.7237 - snr: 2.9053 - val_loss: 0.2244 - val_cc: 0.7380 - val_rrmse: 0.6594 - val_snr: 3.8150\n",
      "Epoch 11/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.2534 - cc: 0.7127 - rrmse: 0.6915 - snr: 3.3484 - val_loss: 0.2094 - val_cc: 0.7580 - val_rrmse: 0.6335 - val_snr: 4.2268\n",
      "Epoch 12/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.2387 - cc: 0.7272 - rrmse: 0.6764 - snr: 3.5828 - val_loss: 0.1951 - val_cc: 0.7768 - val_rrmse: 0.6063 - val_snr: 4.6707\n",
      "Epoch 13/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.2252 - cc: 0.7375 - rrmse: 0.6645 - snr: 3.7344 - val_loss: 0.1826 - val_cc: 0.7925 - val_rrmse: 0.5841 - val_snr: 5.0488\n",
      "Epoch 14/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.2144 - cc: 0.7639 - rrmse: 0.6353 - snr: 4.1636 - val_loss: 0.1726 - val_cc: 0.8059 - val_rrmse: 0.5659 - val_snr: 5.3638\n",
      "Epoch 15/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.2030 - cc: 0.7752 - rrmse: 0.6213 - snr: 4.3388 - val_loss: 0.1632 - val_cc: 0.8170 - val_rrmse: 0.5486 - val_snr: 5.6781\n",
      "Epoch 16/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1937 - cc: 0.7807 - rrmse: 0.6153 - snr: 4.4373 - val_loss: 0.1560 - val_cc: 0.8259 - val_rrmse: 0.5356 - val_snr: 5.9154\n",
      "Epoch 17/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1854 - cc: 0.8037 - rrmse: 0.5823 - snr: 4.9678 - val_loss: 0.1481 - val_cc: 0.8347 - val_rrmse: 0.5199 - val_snr: 6.2228\n",
      "Epoch 18/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.1771 - cc: 0.8013 - rrmse: 0.5912 - snr: 4.7846 - val_loss: 0.1420 - val_cc: 0.8419 - val_rrmse: 0.5069 - val_snr: 6.5077\n",
      "Epoch 19/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.1691 - cc: 0.8144 - rrmse: 0.5700 - snr: 5.1410 - val_loss: 0.1363 - val_cc: 0.8479 - val_rrmse: 0.4972 - val_snr: 6.6853\n",
      "Epoch 20/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1638 - cc: 0.8188 - rrmse: 0.5633 - snr: 5.2485 - val_loss: 0.1321 - val_cc: 0.8536 - val_rrmse: 0.4909 - val_snr: 6.7760\n",
      "Epoch 21/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1575 - cc: 0.8440 - rrmse: 0.5299 - snr: 5.7956 - val_loss: 0.1279 - val_cc: 0.8578 - val_rrmse: 0.4835 - val_snr: 6.9208\n",
      "Epoch 22/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1523 - cc: 0.8309 - rrmse: 0.5476 - snr: 5.4983 - val_loss: 0.1236 - val_cc: 0.8619 - val_rrmse: 0.4721 - val_snr: 7.2099\n",
      "Epoch 23/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1466 - cc: 0.8290 - rrmse: 0.5500 - snr: 5.4667 - val_loss: 0.1207 - val_cc: 0.8652 - val_rrmse: 0.4679 - val_snr: 7.2688\n",
      "Epoch 24/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.1415 - cc: 0.8489 - rrmse: 0.5232 - snr: 5.8740 - val_loss: 0.1171 - val_cc: 0.8689 - val_rrmse: 0.4594 - val_snr: 7.4697\n",
      "Epoch 25/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1373 - cc: 0.8582 - rrmse: 0.5032 - snr: 6.3134 - val_loss: 0.1141 - val_cc: 0.8718 - val_rrmse: 0.4531 - val_snr: 7.6196\n",
      "Epoch 26/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1328 - cc: 0.8650 - rrmse: 0.5021 - snr: 6.2521 - val_loss: 0.1113 - val_cc: 0.8746 - val_rrmse: 0.4476 - val_snr: 7.7530\n",
      "Epoch 27/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.1282 - cc: 0.8616 - rrmse: 0.5065 - snr: 6.1500 - val_loss: 0.1094 - val_cc: 0.8770 - val_rrmse: 0.4442 - val_snr: 7.8035\n",
      "Epoch 28/200\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.1249 - cc: 0.8764 - rrmse: 0.4806 - snr: 6.6613 - val_loss: 0.1069 - val_cc: 0.8789 - val_rrmse: 0.4384 - val_snr: 7.9702\n",
      "Epoch 29/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.1214 - cc: 0.8754 - rrmse: 0.4798 - snr: 6.6604 - val_loss: 0.1048 - val_cc: 0.8816 - val_rrmse: 0.4337 - val_snr: 8.0754\n",
      "Epoch 30/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1179 - cc: 0.8780 - rrmse: 0.4782 - snr: 6.6744 - val_loss: 0.1025 - val_cc: 0.8835 - val_rrmse: 0.4298 - val_snr: 8.1551\n",
      "Epoch 31/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1141 - cc: 0.8827 - rrmse: 0.4725 - snr: 6.7942 - val_loss: 0.1007 - val_cc: 0.8852 - val_rrmse: 0.4271 - val_snr: 8.1978\n",
      "Epoch 32/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.1115 - cc: 0.8879 - rrmse: 0.4609 - snr: 6.9866 - val_loss: 0.0993 - val_cc: 0.8864 - val_rrmse: 0.4225 - val_snr: 8.3700\n",
      "Epoch 33/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1081 - cc: 0.8831 - rrmse: 0.4697 - snr: 6.8580 - val_loss: 0.0973 - val_cc: 0.8886 - val_rrmse: 0.4177 - val_snr: 8.4860\n",
      "Epoch 34/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1050 - cc: 0.8998 - rrmse: 0.4405 - snr: 7.3969 - val_loss: 0.0956 - val_cc: 0.8904 - val_rrmse: 0.4141 - val_snr: 8.5550\n",
      "Epoch 35/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.1019 - cc: 0.9048 - rrmse: 0.4300 - snr: 7.6072 - val_loss: 0.0948 - val_cc: 0.8919 - val_rrmse: 0.4106 - val_snr: 8.6796\n",
      "Epoch 36/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0995 - cc: 0.8997 - rrmse: 0.4381 - snr: 7.4557 - val_loss: 0.0940 - val_cc: 0.8932 - val_rrmse: 0.4085 - val_snr: 8.7665\n",
      "Epoch 37/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0970 - cc: 0.9060 - rrmse: 0.4277 - snr: 7.6330 - val_loss: 0.0920 - val_cc: 0.8948 - val_rrmse: 0.4053 - val_snr: 8.7877\n",
      "Epoch 38/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0937 - cc: 0.9055 - rrmse: 0.4311 - snr: 7.5814 - val_loss: 0.0908 - val_cc: 0.8959 - val_rrmse: 0.4018 - val_snr: 8.9029\n",
      "Epoch 39/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0916 - cc: 0.9074 - rrmse: 0.4198 - snr: 7.8779 - val_loss: 0.0898 - val_cc: 0.8976 - val_rrmse: 0.3988 - val_snr: 8.9767\n",
      "Epoch 40/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0892 - cc: 0.9099 - rrmse: 0.4188 - snr: 7.8501 - val_loss: 0.0885 - val_cc: 0.8993 - val_rrmse: 0.3954 - val_snr: 9.0485\n",
      "Epoch 41/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0874 - cc: 0.9164 - rrmse: 0.4063 - snr: 8.1273 - val_loss: 0.0874 - val_cc: 0.8999 - val_rrmse: 0.3932 - val_snr: 9.1315\n",
      "Epoch 42/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0849 - cc: 0.9200 - rrmse: 0.3981 - snr: 8.3082 - val_loss: 0.0866 - val_cc: 0.9007 - val_rrmse: 0.3917 - val_snr: 9.1625\n",
      "Epoch 43/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0832 - cc: 0.9249 - rrmse: 0.3898 - snr: 8.4319 - val_loss: 0.0858 - val_cc: 0.9020 - val_rrmse: 0.3893 - val_snr: 9.2428\n",
      "Epoch 44/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0818 - cc: 0.9264 - rrmse: 0.3885 - snr: 8.4679 - val_loss: 0.0866 - val_cc: 0.9029 - val_rrmse: 0.3899 - val_snr: 9.2703\n",
      "Epoch 45/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0794 - cc: 0.9269 - rrmse: 0.3849 - snr: 8.5789 - val_loss: 0.0837 - val_cc: 0.9038 - val_rrmse: 0.3844 - val_snr: 9.3576\n",
      "Epoch 46/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0780 - cc: 0.9262 - rrmse: 0.3859 - snr: 8.5959 - val_loss: 0.0844 - val_cc: 0.9054 - val_rrmse: 0.3834 - val_snr: 9.4333\n",
      "Epoch 47/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0762 - cc: 0.9285 - rrmse: 0.3817 - snr: 8.6720 - val_loss: 0.0836 - val_cc: 0.9057 - val_rrmse: 0.3817 - val_snr: 9.4930\n",
      "Epoch 48/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0747 - cc: 0.9303 - rrmse: 0.3797 - snr: 8.6768 - val_loss: 0.0827 - val_cc: 0.9067 - val_rrmse: 0.3790 - val_snr: 9.5687\n",
      "Epoch 49/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0731 - cc: 0.9305 - rrmse: 0.3746 - snr: 8.8510 - val_loss: 0.0831 - val_cc: 0.9075 - val_rrmse: 0.3787 - val_snr: 9.5966\n",
      "Epoch 50/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0717 - cc: 0.9362 - rrmse: 0.3652 - snr: 9.0191 - val_loss: 0.0820 - val_cc: 0.9089 - val_rrmse: 0.3756 - val_snr: 9.6679\n",
      "Epoch 51/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0705 - cc: 0.9351 - rrmse: 0.3653 - snr: 9.0539 - val_loss: 0.0815 - val_cc: 0.9085 - val_rrmse: 0.3756 - val_snr: 9.6679\n",
      "Epoch 52/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0695 - cc: 0.9361 - rrmse: 0.3635 - snr: 9.0696 - val_loss: 0.0810 - val_cc: 0.9100 - val_rrmse: 0.3731 - val_snr: 9.7375\n",
      "Epoch 53/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0681 - cc: 0.9364 - rrmse: 0.3607 - snr: 9.1728 - val_loss: 0.0808 - val_cc: 0.9096 - val_rrmse: 0.3725 - val_snr: 9.7793\n",
      "Epoch 54/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0668 - cc: 0.9354 - rrmse: 0.3683 - snr: 8.9981 - val_loss: 0.0812 - val_cc: 0.9111 - val_rrmse: 0.3725 - val_snr: 9.7862\n",
      "Epoch 55/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0658 - cc: 0.9355 - rrmse: 0.3657 - snr: 9.0379 - val_loss: 0.0829 - val_cc: 0.9109 - val_rrmse: 0.3769 - val_snr: 9.6875\n",
      "Epoch 56/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0648 - cc: 0.9404 - rrmse: 0.3569 - snr: 9.2420 - val_loss: 0.0810 - val_cc: 0.9109 - val_rrmse: 0.3715 - val_snr: 9.8380\n",
      "Epoch 57/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0638 - cc: 0.9425 - rrmse: 0.3463 - snr: 9.5897 - val_loss: 0.0832 - val_cc: 0.9115 - val_rrmse: 0.3775 - val_snr: 9.6953\n",
      "Epoch 58/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0629 - cc: 0.9454 - rrmse: 0.3430 - snr: 9.5848 - val_loss: 0.0801 - val_cc: 0.9123 - val_rrmse: 0.3685 - val_snr: 9.9309\n",
      "Epoch 59/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0618 - cc: 0.9450 - rrmse: 0.3414 - snr: 9.6828 - val_loss: 0.0827 - val_cc: 0.9125 - val_rrmse: 0.3734 - val_snr: 9.8241\n",
      "Epoch 60/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0611 - cc: 0.9439 - rrmse: 0.3432 - snr: 9.5968 - val_loss: 0.0809 - val_cc: 0.9121 - val_rrmse: 0.3698 - val_snr: 9.9266\n",
      "Epoch 61/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0603 - cc: 0.9457 - rrmse: 0.3385 - snr: 9.7400 - val_loss: 0.0806 - val_cc: 0.9130 - val_rrmse: 0.3686 - val_snr: 9.9556\n",
      "Epoch 62/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0592 - cc: 0.9449 - rrmse: 0.3403 - snr: 9.7149 - val_loss: 0.0814 - val_cc: 0.9140 - val_rrmse: 0.3698 - val_snr: 9.9326\n",
      "Epoch 63/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0590 - cc: 0.9470 - rrmse: 0.3372 - snr: 9.8179 - val_loss: 0.0803 - val_cc: 0.9142 - val_rrmse: 0.3671 - val_snr: 10.0070\n",
      "Epoch 64/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0577 - cc: 0.9470 - rrmse: 0.3376 - snr: 9.7337 - val_loss: 0.0802 - val_cc: 0.9146 - val_rrmse: 0.3656 - val_snr: 10.0662\n",
      "Epoch 65/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0573 - cc: 0.9460 - rrmse: 0.3365 - snr: 9.8216 - val_loss: 0.0802 - val_cc: 0.9151 - val_rrmse: 0.3660 - val_snr: 10.0647\n",
      "Epoch 66/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0564 - cc: 0.9483 - rrmse: 0.3278 - snr: 10.0204 - val_loss: 0.0831 - val_cc: 0.9150 - val_rrmse: 0.3732 - val_snr: 9.8274\n",
      "Epoch 67/200\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0556 - cc: 0.9504 - rrmse: 0.3329 - snr: 9.8682 - val_loss: 0.0792 - val_cc: 0.9155 - val_rrmse: 0.3631 - val_snr: 10.1240\n",
      "Epoch 68/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0550 - cc: 0.9514 - rrmse: 0.3251 - snr: 10.0795 - val_loss: 0.0813 - val_cc: 0.9152 - val_rrmse: 0.3670 - val_snr: 10.0299\n",
      "Epoch 69/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0542 - cc: 0.9556 - rrmse: 0.3138 - snr: 10.3519 - val_loss: 0.0809 - val_cc: 0.9159 - val_rrmse: 0.3662 - val_snr: 10.0536\n",
      "Epoch 70/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0540 - cc: 0.9505 - rrmse: 0.3250 - snr: 10.1379 - val_loss: 0.0817 - val_cc: 0.9159 - val_rrmse: 0.3684 - val_snr: 10.0017\n",
      "Epoch 71/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0534 - cc: 0.9552 - rrmse: 0.3137 - snr: 10.4178 - val_loss: 0.0811 - val_cc: 0.9158 - val_rrmse: 0.3676 - val_snr: 10.0265\n",
      "Epoch 72/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0529 - cc: 0.9566 - rrmse: 0.3162 - snr: 10.3180 - val_loss: 0.0827 - val_cc: 0.9160 - val_rrmse: 0.3710 - val_snr: 9.9137\n",
      "Epoch 73/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0525 - cc: 0.9523 - rrmse: 0.3205 - snr: 10.2880 - val_loss: 0.0822 - val_cc: 0.9167 - val_rrmse: 0.3688 - val_snr: 9.9754\n",
      "Epoch 74/200\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0518 - cc: 0.9548 - rrmse: 0.3144 - snr: 10.4298 - val_loss: 0.0799 - val_cc: 0.9171 - val_rrmse: 0.3626 - val_snr: 10.1822\n",
      "Epoch 75/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0512 - cc: 0.9562 - rrmse: 0.3207 - snr: 10.2307 - val_loss: 0.0811 - val_cc: 0.9171 - val_rrmse: 0.3648 - val_snr: 10.1152\n",
      "Epoch 76/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0512 - cc: 0.9578 - rrmse: 0.3025 - snr: 10.7240 - val_loss: 0.0833 - val_cc: 0.9170 - val_rrmse: 0.3706 - val_snr: 9.9442\n",
      "Epoch 77/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0502 - cc: 0.9530 - rrmse: 0.3187 - snr: 10.3102 - val_loss: 0.0812 - val_cc: 0.9177 - val_rrmse: 0.3650 - val_snr: 10.1214\n",
      "Epoch 78/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0500 - cc: 0.9565 - rrmse: 0.3139 - snr: 10.4676 - val_loss: 0.0845 - val_cc: 0.9175 - val_rrmse: 0.3744 - val_snr: 9.8063\n",
      "Epoch 79/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0495 - cc: 0.9581 - rrmse: 0.3068 - snr: 10.6042 - val_loss: 0.0834 - val_cc: 0.9176 - val_rrmse: 0.3711 - val_snr: 9.9321\n",
      "Epoch 80/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0489 - cc: 0.9608 - rrmse: 0.3020 - snr: 10.7692 - val_loss: 0.0834 - val_cc: 0.9179 - val_rrmse: 0.3703 - val_snr: 9.9404\n",
      "Epoch 81/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0485 - cc: 0.9559 - rrmse: 0.3121 - snr: 10.5261 - val_loss: 0.0824 - val_cc: 0.9176 - val_rrmse: 0.3684 - val_snr: 10.0116\n",
      "Epoch 82/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0481 - cc: 0.9544 - rrmse: 0.3162 - snr: 10.3927 - val_loss: 0.0820 - val_cc: 0.9184 - val_rrmse: 0.3675 - val_snr: 10.0196\n",
      "Epoch 83/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0477 - cc: 0.9586 - rrmse: 0.3026 - snr: 10.7872 - val_loss: 0.0850 - val_cc: 0.9183 - val_rrmse: 0.3755 - val_snr: 9.7818\n",
      "Epoch 84/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0474 - cc: 0.9584 - rrmse: 0.3083 - snr: 10.5408 - val_loss: 0.0816 - val_cc: 0.9186 - val_rrmse: 0.3662 - val_snr: 10.0960\n",
      "Epoch 85/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0470 - cc: 0.9633 - rrmse: 0.2941 - snr: 10.9332 - val_loss: 0.0849 - val_cc: 0.9193 - val_rrmse: 0.3751 - val_snr: 9.7899\n",
      "Epoch 86/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0466 - cc: 0.9626 - rrmse: 0.2878 - snr: 11.1803 - val_loss: 0.0833 - val_cc: 0.9188 - val_rrmse: 0.3701 - val_snr: 9.9565\n",
      "Epoch 87/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0463 - cc: 0.9601 - rrmse: 0.3028 - snr: 10.7761 - val_loss: 0.0841 - val_cc: 0.9187 - val_rrmse: 0.3730 - val_snr: 9.8414\n",
      "Epoch 88/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0463 - cc: 0.9590 - rrmse: 0.3019 - snr: 10.7933 - val_loss: 0.0835 - val_cc: 0.9192 - val_rrmse: 0.3710 - val_snr: 9.9235\n",
      "Epoch 89/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0457 - cc: 0.9631 - rrmse: 0.2939 - snr: 10.9760 - val_loss: 0.0823 - val_cc: 0.9195 - val_rrmse: 0.3673 - val_snr: 10.0366\n",
      "Epoch 90/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0455 - cc: 0.9630 - rrmse: 0.2939 - snr: 11.0229 - val_loss: 0.0838 - val_cc: 0.9190 - val_rrmse: 0.3716 - val_snr: 9.8545\n",
      "Epoch 91/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0452 - cc: 0.9644 - rrmse: 0.2911 - snr: 11.0731 - val_loss: 0.0860 - val_cc: 0.9192 - val_rrmse: 0.3763 - val_snr: 9.7431\n",
      "Epoch 92/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0450 - cc: 0.9636 - rrmse: 0.2919 - snr: 11.1408 - val_loss: 0.0873 - val_cc: 0.9193 - val_rrmse: 0.3818 - val_snr: 9.5537\n",
      "Epoch 93/200\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0448 - cc: 0.9635 - rrmse: 0.2927 - snr: 11.0585 - val_loss: 0.0851 - val_cc: 0.9194 - val_rrmse: 0.3755 - val_snr: 9.7703\n",
      "Epoch 94/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0445 - cc: 0.9643 - rrmse: 0.2937 - snr: 10.9415Restoring model weights from the end of the best epoch: 74.\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0445 - cc: 0.9631 - rrmse: 0.2903 - snr: 11.1515 - val_loss: 0.0869 - val_cc: 0.9199 - val_rrmse: 0.3805 - val_snr: 9.5800\n",
      "Epoch 94: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, CSVLogger\n",
    "\n",
    "from metrics import CC, RRMSE, SNR\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=5e-5, epsilon=1e-8),\n",
    "    loss=MeanSquaredError(),\n",
    "    metrics=[CC(), RRMSE(), SNR()],\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_snr',\n",
    "        mode='max',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'{output_dir}/novel_cnn.h5',\n",
    "        mode='max',\n",
    "        monitor='val_snr',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    ),\n",
    "    TensorBoard(log_dir=output_dir),\n",
    "    CSVLogger(f'{output_dir}/novel_cnn.csv'),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=200,\n",
    "    steps_per_epoch=200,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 10ms/step - loss: 0.0821 - cc: 0.9200 - rrmse: 0.3505 - snr: 10.5157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08205802738666534,\n",
       " 0.9199916124343872,\n",
       " 0.35051077604293823,\n",
       " 10.515700340270996]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
